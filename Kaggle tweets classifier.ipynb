{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'cleaning.py',\n",
       " 'glove_word_embedding',\n",
       " 'Kaggle tweets classifier.ipynb',\n",
       " 'model.h5',\n",
       " 'model.json',\n",
       " 'model_conv.h5',\n",
       " 'model_conv.json',\n",
       " 'model_conv_bid.h5',\n",
       " 'model_conv_bid.json',\n",
       " 'Output.txt',\n",
       " 'sample_submission.csv',\n",
       " 'sample_submission_kaggle.csv',\n",
       " 'submission.csv',\n",
       " 'tensorflow_self_check.py',\n",
       " 'tensorflow_test.py',\n",
       " 'test.csv',\n",
       " 'test.ipynb',\n",
       " 'train.csv',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (BatchNormalization, Conv1D, Dense, Input,SpatialDropout1D,\n",
    "    TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM, MaxPooling1D, Embedding)\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "from cleaning import * #function *clean*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ivanm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14330125163319129851\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1444338073\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11906247219588722507\n",
      "physical_device_desc: \"device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7613 rows and 5 columns in train\n",
      "There are 3263 rows and 4 columns in train\n"
     ]
    }
   ],
   "source": [
    "#uploading data\n",
    "tweet = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\n",
    "print('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'samples')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=tweet.target.value_counts()\n",
    "sns.barplot(x.index,x)\n",
    "plt.gca().set_ylabel('samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Informatik\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10876, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat([tweet,test])\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New competition launched :'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\"\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "remove_URL(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_URL(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"<div>\n",
    "<h1>Real or Fake</h1>\n",
    "<p>Kaggle </p>\n",
    "<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n",
    "</div>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real or Fake\n",
      "Kaggle \n",
      "getting started\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "print(remove_html(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Omg another Earthquake '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoji(\"Omg another Earthquake 😔😔\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a king\n"
     ]
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "example=\"I am a #king\"\n",
    "print(remove_punct(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_punct(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'correct me please'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "        \n",
    "text = \"corect me plese\"\n",
    "correct_spellings(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"text\"]=df[\"text\"].apply(lambda x : correct_spellings(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving text for BERT\n",
    "\n",
    "text_file = open(\"Output.txt\", \"w\")\n",
    "text_file.write(str(df[\"text\"]))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df['text']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "        corpus.append(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10876/10876 [00:02<00:00, 4945.59it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus=create_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict={}\n",
    "with open('glove_word_embedding/glove.6B.200d.txt','r',encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 20169\n"
     ]
    }
   ],
   "source": [
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 20169/20169 [00:00<00:00, 493253.32it/s]\n"
     ]
    }
   ],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,200))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec=embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "def load_model(path_json, path_h5):\n",
    "    # load json and create model\n",
    "    json_file = open(path_json, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(path_h5)\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model(\"model_conv.json\",\"model_conv.h5\")\n",
    "model = loaded_model\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "\n",
    "embedding=Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(Conv1D(200, 11, \n",
    "                     strides=2, \n",
    "                     padding=\"valid\",\n",
    "                     activation='tanh',   #conv\n",
    "                     name='cnn_1'))\n",
    "# batch normalization\n",
    "model.add(BatchNormalization(name='bn_cnn_1'))\n",
    "# max pooling\n",
    "model.add(MaxPooling1D(2, name=f'maxp_cnn'))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences = True))\n",
    "model.add(GRU(32, dropout=0.1, recurrent_dropout=0.2, return_sequences = True))\n",
    "# test\n",
    "model.add(Bidirectional(GRU(32, dropout=0.1, recurrent_dropout=0.2, return_sequences = True), merge_mode='concat'))\n",
    "model.add(LSTM(32, dropout=0.2, activation=\"relu\", recurrent_dropout=0.2, return_sequences = True))\n",
    "model.add(Bidirectional(GRU(16, dropout=0.1, recurrent_dropout=0.2, return_sequences = False), merge_mode='concat'))\n",
    "\n",
    "model.add(BatchNormalization(name=\"bn_rnn\"))\n",
    "# TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Bidirectional(rnn, merge_mode='concat')\n",
    "# model.add(bert_layer)\n",
    "\n",
    "\n",
    "optimizer=Adam(learning_rate=1e-5)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 200)           4034000   \n",
      "_________________________________________________________________\n",
      "cnn_1 (Conv1D)               (None, 20, 200)           440200    \n",
      "_________________________________________________________________\n",
      "bn_cnn_1 (BatchNormalization (None, 20, 200)           800       \n",
      "_________________________________________________________________\n",
      "maxp_cnn (MaxPooling1D)      (None, 10, 200)           0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 10, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 64)            67840     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 10, 32)            9312      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 10, 64)            12480     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 10, 32)            12416     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 32)                4704      \n",
      "_________________________________________________________________\n",
      "bn_rnn (BatchNormalization)  (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,581,913\n",
      "Trainable params: 547,449\n",
      "Non-trainable params: 4,034,464\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tweet_pad[:tweet.shape[0]]\n",
    "test=tweet_pad[tweet.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train (6851, 50)\n",
      "Shape of Validation  (762, 50)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.1)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/5\n",
      " - 387s - loss: 0.7141 - accuracy: 0.5566 - val_loss: 0.6080 - val_accuracy: 0.7047\n",
      "Epoch 2/5\n",
      " - 411s - loss: 0.6676 - accuracy: 0.6075 - val_loss: 0.5727 - val_accuracy: 0.7310\n",
      "Epoch 3/5\n",
      " - 487s - loss: 0.6406 - accuracy: 0.6398 - val_loss: 0.5525 - val_accuracy: 0.7546\n",
      "Epoch 4/5\n",
      " - 389s - loss: 0.6364 - accuracy: 0.6449 - val_loss: 0.5450 - val_accuracy: 0.7454\n",
      "Epoch 5/5\n",
      " - 400s - loss: 0.6258 - accuracy: 0.6625 - val_loss: 0.5415 - val_accuracy: 0.7533\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=2,epochs=5,validation_data=(X_test,y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_conv_bid.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_conv_bid.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub=pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "\n",
    "y_pre=model.predict(test)\n",
    "y_pre=np.round(y_pre).astype(int).reshape(3263)\n",
    "sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3258</td>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3259</td>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3261</td>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3262</td>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    \n",
    "    if Dropout_num == 0:\n",
    "        # Without Dropout\n",
    "        out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    else:\n",
    "        # With Dropout(Dropout_num), Dropout_num > 0\n",
    "        x = Dropout(Dropout_num)(clf_output)\n",
    "        out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(bert_layer, max_len=512):\n",
    "#     input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "#     input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "#     segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "#     _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "#     clf_output = sequence_output[:, 0, :]\n",
    "#     out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "#     model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "#     model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big target correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "    \"\"\"Removes links and non-ASCII characters\"\"\"\n",
    "    \n",
    "    tweet = ''.join([x for x in tweet if x in string.printable])\n",
    "    \n",
    "    # Removing URLs\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    \n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "\n",
    "    text = text.replace('...', ' ... ')\n",
    "    \n",
    "    if '...' not in text:\n",
    "        text = text.replace('..', ' ... ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"€\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "    \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_abbrev(word):\n",
    "    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_abbrev_in_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [convert_abbrev(word) for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### target correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_corrected:\n",
    "    ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "    train.loc[train['id'].isin(ids_with_target_error),'target'] = 0\n",
    "    train[train['id'].isin(ids_with_target_error)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
